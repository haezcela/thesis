import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Flatten
from keras.callbacks import Callback
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import nltk

nltk.download('punkt')
from nltk.tokenize import word_tokenize

# Load the CSV dataset
df = pd.read_csv('/content/drive/MyDrive/dataset/cases_dataset_reduced.csv')

# Tokenize the text data
df['Decision_Tokens'] = df['Decision'].apply(lambda x: word_tokenize(x.lower()) if pd.notnull(x) else [])

# Separate affirmed and non-affirmed texts
affirmed_texts = df[df['Verdict'] == 'affirmed']['Decision_Tokens'].tolist()
non_affirmed_texts = df[df['Verdict'] == 'non-affirmed']['Decision_Tokens'].tolist()

# Train Word2Vec models for affirmed and non-affirmed texts
affirmed_model = Word2Vec(affirmed_texts, vector_size=100, window=5, min_count=1)
non_affirmed_model = Word2Vec(non_affirmed_texts, vector_size=100, window=5, min_count=1)

# Combine the models manually
combined_vocab = set(affirmed_model.wv.index_to_key).union(set(non_affirmed_model.wv.index_to_key))
embedding_matrix = np.zeros((len(combined_vocab) + 1, 100))  # +1 for padding index 0
word_index = {word: idx + 1 for idx, word in enumerate(combined_vocab)}  # Start indices from 1

for word, idx in word_index.items():
    if word in affirmed_model.wv:
        embedding_matrix[idx] = affirmed_model.wv[word]
    elif word in non_affirmed_model.wv:
        embedding_matrix[idx] = non_affirmed_model.wv[word]

# Preprocess the text data
def preprocess_text(texts, word_index, maxlen=100):
    tokenized_texts = [[word_index.get(word, 0) for word in text] for text in texts]
    padded_sequences = pad_sequences(tokenized_texts, maxlen=maxlen)
    return padded_sequences

affirmed_sequences = preprocess_text(affirmed_texts, word_index)
non_affirmed_sequences = preprocess_text(non_affirmed_texts, word_index)

# Combine the sequences and labels
X = np.concatenate((affirmed_sequences, non_affirmed_sequences))
y = np.concatenate((np.ones(len(affirmed_sequences)), np.zeros(len(non_affirmed_sequences))))

# To store metrics for plotting
accuracy_history = []
precision_history = []
recall_history = []
f1_history = []
loss_history = []

affirmed_precision_history = []
affirmed_recall_history = []
affirmed_f1_history = []

non_affirmed_precision_history = []
non_affirmed_recall_history = []
non_affirmed_f1_history = []

confusion_matrices = []

# Custom callback to compute metrics after each epoch
class MetricsCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        y_pred = (self.model.predict(X) > 0.5).astype("int32")

        loss = logs['loss']
        accuracy = accuracy_score(y, y_pred)
        precision = precision_score(y, y_pred, zero_division=1)
        recall = recall_score(y, y_pred, zero_division=1)
        f1 = f1_score(y, y_pred, zero_division=1)

        accuracy_history.append(accuracy)
        precision_history.append(precision)
        recall_history.append(recall)
        f1_history.append(f1)
        loss_history.append(loss)

        affirmed_precision = precision_score(y, y_pred, pos_label=1, zero_division=1)
        affirmed_recall = recall_score(y, y_pred, pos_label=1, zero_division=1)
        affirmed_f1 = f1_score(y, y_pred, pos_label=1, zero_division=1)

        affirmed_precision_history.append(affirmed_precision)
        affirmed_recall_history.append(affirmed_recall)
        affirmed_f1_history.append(affirmed_f1)

        non_affirmed_precision = precision_score(y, y_pred, pos_label=0, zero_division=1)
        non_affirmed_recall = recall_score(y, y_pred, pos_label=0, zero_division=1)
        non_affirmed_f1 = f1_score(y, y_pred, pos_label=0, zero_division=1)

        non_affirmed_precision_history.append(non_affirmed_precision)
        non_affirmed_recall_history.append(non_affirmed_recall)
        non_affirmed_f1_history.append(non_affirmed_f1)

        cm = confusion_matrix(y, y_pred)
        confusion_matrices.append(cm)

        print(f'Epoch {epoch+1} - Loss: {loss:.4f} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')
        print(classification_report(y, y_pred, zero_division=1))
        print("\nConfusion Matrix:")
        print(cm)

# Train the CNN model
model = Sequential()
model.add(Embedding(input_dim=len(combined_vocab) + 1, output_dim=100, weights=[embedding_matrix], input_length=100, trainable=False))
model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))
model.add(MaxPooling1D(pool_size=5))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X, y, epochs=10, batch_size=32, callbacks=[MetricsCallback()])

# Final evaluation
y_pred = (model.predict(X) > 0.5).astype("int32")
accuracy = accuracy_score(y, y_pred)
precision = precision_score(y, y_pred, zero_division=1)
recall = recall_score(y, y_pred, zero_division=1)
f1 = f1_score(y, y_pred, zero_division=1)

print('Final Metrics:')
print('Accuracy:', accuracy)
print('Precision:', precision)
print('Recall:', recall)
print('F1-score:', f1)
print(classification_report(y, y_pred, zero_division=1))

# Confusion Matrix
cm = confusion_matrix(y, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Plotting the metrics
epochs = range(1, 11)

plt.figure(figsize=(12, 16))

# Overall metrics
plt.subplot(5, 1, 1)
plt.plot(epochs, loss_history, label='Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss per Epoch')
plt.legend()
plt.grid(True)

plt.subplot(5, 1, 2)
plt.plot(epochs, accuracy_history, label='Accuracy')
plt.plot(epochs, precision_history, label='Precision')
plt.plot(epochs, recall_history, label='Recall')
plt.plot(epochs, f1_history, label='F1-score')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.title('Overall Training Metrics per Epoch')
plt.legend()
plt.grid(True)

# Affirmed class metrics
plt.subplot(5, 1, 3)
plt.plot(epochs, affirmed_precision_history, label='Affirmed Precision')
plt.plot(epochs, affirmed_recall_history, label='Affirmed Recall')
plt.plot(epochs, affirmed_f1_history, label='Affirmed F1-score')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.title('Affirmed Class Training Metrics per Epoch')
plt.legend()
plt.grid(True)

# Non-affirmed class metrics
plt.subplot(5, 1, 4)
plt.plot(epochs, non_affirmed_precision_history, label='Non-affirmed Precision')
plt.plot(epochs, non_affirmed_recall_history, label='Non-affirmed Recall')
plt.plot(epochs, non_affirmed_f1_history, label='Non-affirmed F1-score')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.title('Non-affirmed Class Training Metrics per Epoch')
plt.legend()
plt.grid(True)

# Confusion matrix values
tp_values = [cm[1, 1] for cm in confusion_matrices]
tn_values = [cm[0, 0] for cm in confusion_matrices]
fp_values = [cm[0, 1] for cm in confusion_matrices]
fn_values = [cm[1, 0] for cm in confusion_matrices]

plt.subplot(5, 1, 5)
plt.plot(epochs, tp_values, label='True Positives', marker='o')
plt.plot(epochs, tn_values, label='True Negatives', marker='o')
plt.plot(epochs, fp_values, label='False Positives', marker='o')
plt.plot(epochs, fn_values, label='False Negatives', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Confusion Matrix Values')
plt.title('Confusion Matrix Values per Epoch')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Displaying confusion matrices for each epoch
for epoch, cm in enumerate(confusion_matrices, 1):
    print(f'Epoch {epoch} Confusion Matrix:')
    print(cm)
    print()
